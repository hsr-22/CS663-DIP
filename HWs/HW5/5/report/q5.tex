\documentclass{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}
\usepackage{fancyvrb}
\usepackage{enumitem}
\usepackage{matlab-prettifier}
\setlength{\parindent}{0pt}
\graphicspath{{../images/}}

\title{CS663: Digital Image Processing - Homework 5}
\author{Harsh $\vert$ Pranav $\vert$ Swayam} 
\date{November 6, 2024}

\begin{document}

\maketitle
\flushleft
\section*{Homework 5 - Question 5}

\subsection*{Part a - Why the Standard Least Squares Solution Fails}

The least squares solution would suggest solving for \( R \) by minimizing:
\[
\| P_1 - R P_2 \|_F^2,
\]
which leads to:
\[
R = P_1 P_2^T (P_2 P_2^T)^{-1}.
\]
However, this solution does not guarantee that \( R \) will be orthonormal (i.e., \( R^T R = I \)). Without this constraint, \( R \) may not preserve lengths and angles, which is essential in many applications. Therefore, we need to modify our approach to ensure that \( R \) is orthonormal.


\subsection*{Part b - Setting Up the Cost Function and Justifying Steps}

The objective function we aim to minimize is:
\[
E(R) = \| P_1 - R P_2 \|_F^2,
\]
where \( R \) is an orthonormal matrix, meaning \( R^T R = I \).

The Frobenius norm of a matrix \( A \) is defined as:
\[
\| A \|_F^2 = \text{trace}(A^T A).
\]
Therefore, we can rewrite \( E(R) \) as:
\[
E(R) = \text{trace}((P_1 - R P_2)^T (P_1 - R P_2)).
\]

Expanding the term \((P_1 - R P_2)^T (P_1 - R P_2)\), we get:
\[
E(R) = \text{trace}(P_1^T P_1 - P_1^T R P_2 - P_2^T R^T P_1 + P_2^T R^T R P_2).
\]

Since \( R \) is an orthonormal matrix, we know \( R^T R = I \). Thus, the term \( P_2^T R^T R P_2 \) simplifies to \( P_2^T P_2 \). Applying this condition, we obtain:
\[
E(R) = \text{trace}(P_1^T P_1 + P_2^T P_2 - P_1^T R P_2 - P_2^T R^T P_1).
\]

We know that \( \text{trace}(A) = \text{trace}(A^T) \) for any square matrix \( A \). This allows us to combine the last two terms as follows:
\[
E(R) = \text{trace}(P_1^T P_1 + P_2^T P_2) - \text{trace}(P_1^T R P_2 + P_2^T R^T P_1).
\]
Since \( \text{trace}(P_1^T R P_2) = \text{trace}(P_2^T R^T P_1) \), we can rewrite this as:
\[
E(R) = \text{trace}(P_1^T P_1 + P_2^T P_2) - 2 \, \text{trace}(P_1^T R P_2).
\]


\subsection*{Part c - Min \(E(R)\) equivalent to  Max \(trace(P_1^TRP_2)\)}

The expression we derived,
\[
E(R) = \text{trace}(P_1^T P_1 + P_2^T P_2) - 2 \, \text{trace}(P_1^T R P_2),
\]
has two parts:
\begin{itemize}
    \item The term \( \text{trace}(P_1^T P_1 + P_2^T P_2) \) is independent of \( R \). It represents a constant based on the fixed matrices \( P_1 \) and \( P_2 \).
    \item The term \( -2 \, \text{trace}(P_1^T R P_2) \) depends on \( R \). Minimizing \( E(R) \) is therefore equivalent to maximizing \( \text{trace}(P_1^T R P_2) \) with respect to \( R \).
\end{itemize}

Thus, minimizing the objective function \( E(R) \) is equivalent to solving the following problem:
\[
\text{maximize } \text{trace}(P_1^T R P_2) \quad \text{subject to } R^T R = I.
\]

\subsection*{Part d - Detailed Solution with Justification for Each Step}

\( \text{trace}(P_1^T R P_2) = \text{trace}(R P_2 P_1^T) \), this equality holds due to the cyclic property of trace. For any matrices \( A \), \( B \), and \( C \) of compatible dimensions, the trace of a product is invariant under cyclic permutations:
\[
\text{trace}(ABC) = \text{trace}(BCA) = \text{trace}(CAB)
\]
Applying this property, we have:
\[
\text{trace}(P_1^T R P_2) = \text{trace}(R P_2 P_1^T)
\]
This cyclic shift allows us to rewrite the expression in terms of \( R P_2 P_1^T \), which becomes useful in subsequent steps when we apply the SVD.

We proceed by performing the SVD on the matrix \( P_2 P_1^T \), which gives us:
\[
P_2 P_1^T = U' S' V'^T
\]
where \( U' \) and \( V' \) are orthogonal matrices, and \( S' \) is a diagonal matrix with non-negative real entries (the singular values of \( P_2 P_1^T \)).

Now, substituting this into our trace expression, we get:
\[
\text{trace}(R P_2 P_1^T) = \text{trace}(R U' S' V'^T)
\]
This substitution is valid since we are replacing \( P_2 P_1^T \) with its decomposition, allowing us to simplify the trace further.

Using the cyclic property of trace again, we can rearrange terms inside the trace without changing the value:
\[
\text{trace}(R U' S' V'^T) = \text{trace}(S' V'^T R U')
\]
Defining \( X = V'^T R U' \), we get:
\[
\text{trace}(S' V'^T R U') = \text{trace}(S' X)
\]
This expression isolates \( S' \), the singular values, which are instrumental in maximizing the trace expression in subsequent parts.


\subsection*{Part e - Finding the Matrix \( X \) that Maximizes \( \text{trace}(S' X) \)}

We aim to maximize the expression \( \text{trace}(S' X) \) with respect to matrix \( X \), given that \( S' \) is a diagonal matrix with non-negative singular values. Here is how we approach this maximization problem:


Since \( X = V'^T R U' \), and \( R \) is an orthonormal matrix, \( X \) inherits the constraints imposed by \( R \). For the trace to be maximized, we must choose \( R \) (and hence \( X \)) in a way that aligns the entries of \( X \) with the entries of \( S' \).


The trace \( \text{trace}(S' X) \) is maximized when \( X \) matches the identity matrix in terms of the alignment of singular values. Thus, to maximize \( \text{trace}(S' X) \), we need \( X \) to be a rotation matrix that aligns the directions of \( V' \) and \( U' \).


The maximal alignment happens when \( X = I \), as this configuration fully aligns the singular values in \( S' \) with those in \( X \). Thus, the optimal choice for \( X \) is:
\[
X = I
\]


\subsection*{Part f - Determining \( R \) from the Optimal \( X \)}

Since\( X = I \)  this leads to \( V'^T R U' = I \), or equivalently:
   \[
   R = V' U'^T
   \]


\subsection*{Part g - Additional Constraint if \( R \) is a Rotation Matrix}

 A rotation matrix in 2D or 3D has a determinant of +1, as opposed to reflection matrices, which have a determinant of -1. Thus:

\textbf{Enforcing \( R \) as a Rotation Matrix}:
   To ensure \( R \) is a rotation matrix, we need to impose the constraint:
   \[
   \det(R) = +1
   \]


   After computing \( R = V' U'^T \), if we find that \( \det(R) = -1 \), we can adjust \( R \) to make it a rotation matrix by flipping the sign of one of the columns in \( U' \) or \( V' \). This ensures that the determinant becomes +1 while maintaining the orthonormality of \( R \).

\end{document}