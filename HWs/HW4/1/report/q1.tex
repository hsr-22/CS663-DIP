\documentclass{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}
\usepackage{fancyvrb}
\usepackage{enumitem}
\usepackage{matlab-prettifier}
\setlength{\parindent}{0pt}
\graphicspath{{../images/}}

\title{CS663: Digital Image Processing - Homework 4}
\author{Harsh $\vert$ Pranav $\vert$ Swayam} 
\date{October 22, 2024}

\begin{document}

\maketitle
\flushleft
\section*{Homework 4 - Question 1}

\subsection*{Part a -}

PCA matrix is defined as:
\[
C = \frac{1}{N-1} X  X^T
\]


where X is the vector:

\[
X=\begin{bmatrix} x_1 & x_2 \dots x_{n-1} & x_n\end{bmatrix}
\]

\subsubsection*{Symmetry}

For a Matrix m to be symmetric it should satisfy the condition,

\[
M = M^T
\]

so trying that out wit C we get:

\[
C^T = (\frac{1}{N-1} XX^T)^T
\]
\[
C^T = \frac{1}{N-1} (X^T)^T(X)^T
\]
\[
C^T = \frac{1}{N-1} XX^T
\]
Finally,
\[
C^T = C
\]

Hence PCA matrix is symmetric.

\subsubsection*{Semi definite}

For a matrix to to positive semi definite, it should satisfy the equation $v^TMv>=0$ for any vector $v$.

\[
v^TCv = \frac{1}{N-1} v^TXX^Tv
\]
So this can be re written as:
\[
v^TCv = \frac{1}{N-1} \|{v^T X}\|^2
\]

And this quantity is surely greater than equal to 0 so the pcA matrix is positive semi definite.

\subsection*{Part b - }

Let $M$ be a symmetric matrix, and let $v_1$ and $v_2$ be two eigenvectors corresponding to distinct eigenvalues $\lambda_1$ and $\lambda_2$, respectively. We have the following eigenvalue equations:

\[
M v_1 = \lambda_1 v_1 \quad \text{and} \quad M v_2 = \lambda_2 v_2
\]

Taking the inner product of $v_1^T$ with both sides of the second equation:

\[
v_1^T M v_2 = \lambda_2 v_1^T v_2
\]

Since $M$ is symmetric, we know that:

\[
v_1^T M v_2 = (M v_1)^T v_2 = \lambda_1 v_1^T v_2
\]

Thus, we have:

\[
\lambda_1 v_1^T v_2 = \lambda_2 v_1^T v_2
\]

For $\lambda_1 \neq \lambda_2$, the only solution to this equation is:

\[
v_1^T v_2 = 0
\]

This shows that $v_1$ and $v_2$ are orthogonal.

To normalize the eigenvectors, we scale each eigenvector so that:

\[
\|v_i\| = 1 \quad \text{for each eigenvector} \, v_i
\]

Thus, the eigenvectors of the symmetric matrix $C$ are orthonormal.


\subsection*{Part c - }

We are approximating the difference between the original data points \( x_i \) and their truncated versions \( \tilde{x}_i \) by truncating the eigen-coefficients for the smallest eigenvalues.

The difference is given by:

\[
\| x_i - \tilde{x}_i \|^2 = \| V (\alpha_i - \tilde{\alpha}_i) \|^2
\]

where \( \alpha_i \) are the original eigen-coefficients and \( \tilde{\alpha}i \) are the truncated eigen-coefficients. The truncation sets \( \tilde{\alpha}{il} = 0 \) for \( l > k \), so:

\[
\| x_i - \tilde{x}i \|^2 = \sum{l=k+1}^d \alpha_{il}^2
\]

Averaging over all data points gives:

\[
\frac{1}{N} \sum_{i=1}^N \| x_i - \tilde{x}i \|^2 = \frac{1}{N} \sum{i=1}^N \sum_{l=k+1}^d \alpha_{il}^2
\]

The eigen-coefficients \( \alpha_{il} \) have variance \( \lambda_l \), the eigenvalue corresponding to the \( l \)-th eigenvector. Thus, the expected squared difference is approximately:

\[
\sum_{l=k+1}^d \lambda_l
\]

This is small when the eigenvalues \( \lambda_{k+1}, \lambda_{k+2}, \dots, \lambda_d \) are small, as they represent the least significant components of the data's variation.

\subsection*{Part  -}

The problem describes a situation where one variable (denoted as \( X_1 \)) has a much larger variance than the other (denoted as \( X_2 \)). This means that the covariance matrix of the data has one large eigenvalue (corresponding to the direction of \( X_1 \)) and one small eigenvalue (corresponding to \( X_2 \)).

The eigenvectors are:

\[
\text{For the larger eigenvalue (100):} \quad (1; 0)
\]
\[
\text{For the smaller eigenvalue (1):} \quad (0; 1)
\]

Thus, the covariance matrix in this case has the form:

\[
\begin{pmatrix}
100 & 0 \\
0 & 1
\end{pmatrix}
\]

where 100 and 1 are the eigenvalues corresponding to the directions of \( X_1 \) and \( X_2 \), respectively.


\end{document}